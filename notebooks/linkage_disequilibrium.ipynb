{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simulating Linkage Disequilibrium\n",
        "\n",
        "Matthew Bracher-Smith"
      ],
      "id": "2d1dbf06-2ffa-4328-a51e-a7100924da4b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(dplyr)\n",
        "library(stringr)\n",
        "library(readr)\n",
        "library(glmnet)\n",
        "library(ggcorrplot)\n",
        "library(glmnet)\n",
        "library(pROC)\n",
        "library(here)\n",
        "set.seed(123)"
      ],
      "id": "6e8b201e-195f-4833-9c03-acbeab6a4748"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulating unassociated SNPs\n",
        "\n",
        "-   We covered additive risk variants under a liability-threshold model\n",
        "-   We also added multiplicative interactions between two loci\n",
        "-   But in fact we can have any relationships between two SNPs\n",
        "    (theoretically)\n",
        "\n",
        "Start by setting up functions for unassociated SNPs as before:"
      ],
      "id": "ad7881ea-0063-41d9-9aba-7396b1fde7e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n <- 10000 # observations\n",
        "p <- 100 # SNPs"
      ],
      "id": "801e9078-2ad3-4c26-86cd-9ee2f2978bc1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then generate unassociated genotypes"
      ],
      "id": "59394d56-bb94-4a01-a6a0-e52d1dedac5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "add_noise <- function(n = 1000, p = 100, maf_range = c(0.05, 0.5)) {\n",
        "  maf <- runif(p, maf_range[1], maf_range[2])\n",
        "  genotypes <- sapply(maf, function(freq) rbinom(n, 2, freq))\n",
        "  return(genotypes)\n",
        "}"
      ],
      "id": "10e165ff-d459-4529-90c0-ea5d322b25af"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And run a quick check that everything’s okay:"
      ],
      "id": "1e664b25-c18d-42d7-8469-340ba8638928"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n",
            " [1,]    0    0    1    1    1    0    0    1    0     0\n",
            " [2,]    0    0    0    1    1    1    1    1    1     1\n",
            " [3,]    0    1    0    1    0    1    2    2    0     1\n",
            " [4,]    1    0    0    0    0    0    0    2    0     0\n",
            " [5,]    0    1    1    0    0    0    0    0    0     0\n",
            " [6,]    1    1    0    0    0    0    0    0    1     0\n",
            " [7,]    1    1    1    2    0    0    0    2    2     1\n",
            " [8,]    0    1    1    1    1    0    1    0    1     0\n",
            " [9,]    0    1    1    0    1    0    1    1    0     1\n",
            "[10,]    0    2    0    2    1    0    0    1    2     0"
          ]
        }
      ],
      "source": [
        "g <- add_noise(n, p)\n",
        "g[1:10, 1:10]"
      ],
      "id": "7c9375ad-23d2-49da-bf97-eb9d20fa3104"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulating simple LD\n",
        "\n",
        "We’ll measure this here using pearsons correlation (though there are\n",
        "other prominent metrics like D\\`) making SNPs in varying LD is actually\n",
        "really easy:\n",
        "\n",
        "-   If we duplicated a SNP, then correlation (LD) would be perfect\n",
        "-   To decrease this correlation, we randomly shuffle a proportion of\n",
        "    the rows in one of the SNPs\n",
        "-   The greater proportion of rows that we shuffle, the more\n",
        "    decorrelated they are\n",
        "-   Shuffling all would be linkage equilibrium and the SNPs would not be\n",
        "    correlated anymore\n",
        "-   Randomly shuffling (rather than flipping them from e.g. 2 to 1 or 0)\n",
        "    means we maintain MAF and HWE too\n",
        "\n",
        "For a given SNP, we can make a couple of functions to derive simple LD,\n",
        "LD blocks and more.\n",
        "\n",
        "### Replacing causal SNPs with SNPs in LD\n",
        "\n",
        "This function returns a SNP in LD with the SNP we pass in"
      ],
      "id": "9a6fc995-d638-4217-abf2-3b08f708a134"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "add_simple_ld <- function(causal_snp, r2) {\n",
        "  indices_to_shuffle <- sample(1:length(causal_snp),\n",
        "    size = (1 - r2) * length(causal_snp),\n",
        "    replace = FALSE\n",
        "  )\n",
        "  ld_snp <- causal_snp\n",
        "\n",
        "  if (length(indices_to_shuffle) > 1) {\n",
        "    shuffledpeople <- sample(causal_snp[indices_to_shuffle])\n",
        "    ld_snp[indices_to_shuffle] <- shuffledpeople\n",
        "  }\n",
        "\n",
        "\n",
        "  return(ld_snp)\n",
        "}"
      ],
      "id": "4801abd1-3827-4b6d-9f23-64cf17c39eea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, this function takes genotypes, a proportion of indices to sample\n",
        "and r2 values and replaces all the SNPs at those indices with the\n",
        "associated r2 values"
      ],
      "id": "a1cad6b1-7801-44ec-850f-13ae897731a5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "replace_with_ld <- function(g, m, ld_range = c(0.1, 0.9)) {\n",
        "  # g is our genotypes\n",
        "  # m is the proportion of causal SNPs we want to keep\n",
        "  # ld_range is the min/max values for a uniform distribution\n",
        "  # for us to sample from for\n",
        "  #          the LD for the new SNPs we will create\n",
        "  #          Note we replace the causal SNPs with the LD ones\n",
        "  #          (rather than adding them in)\n",
        "  #          This has the added benefit of stopping dimension size\n",
        "  #          from blowing-up\n",
        "  snp_indices_to_replace <- sample(1:dim(g)[2],\n",
        "    as.integer((1 - m) * dim(g)[2]),\n",
        "    replace = FALSE\n",
        "  )\n",
        "  ld_for_replacements <- runif(\n",
        "    length(snp_indices_to_replace),\n",
        "    ld_range[1], ld_range[2]\n",
        "  )\n",
        "\n",
        "  for (i in 1:length(snp_indices_to_replace)) {\n",
        "    j <- snp_indices_to_replace[i]\n",
        "    g[, j] <- add_simple_ld(g[, j], ld_for_replacements[i])\n",
        "  }\n",
        "\n",
        "  return(g)\n",
        "}"
      ],
      "id": "72a08ae5-f6bc-4af7-a04c-b6c146d96fba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running a quick check\n",
        "\n",
        "-   We create new genotypes h where there’s only 10% of the causal SNPs\n",
        "    tagged\n",
        "-   Here they’re all noise, so nothing’s really causal though"
      ],
      "id": "df1f0653-b77e-4453-9f92-9d48232a613f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h <- replace_with_ld(g, m = 0.1)"
      ],
      "id": "efe6b22f-5416-4e54-bf34-8f17ac454cbe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "# derive a genotypic score (PRS) for each\n",
        "# here this is just the unweighted sum of the alleles\n",
        "# there are no risk alleles here as they're all unassociated, but it allows us\n",
        "# to reduce the dimensions to compare two datasets easily\n",
        "plot(colSums(g), colSums(h))"
      ],
      "id": "4380e14e-670a-46a6-b6f4-8c7567447bdd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 0.6860844"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 0.5647958"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 0.8096285"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 0.5248311"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 0.5447041"
          ]
        }
      ],
      "source": [
        "# but individual genotypes should be less correlated (90% of the time)\n",
        "cor(g[, 1], h[, 1])"
      ],
      "id": "c21d8dc8-a83b-4322-b531-bb34059d9efe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulating more complex LD blocks\n",
        "\n",
        "Again, suprisingly simple to do:\n",
        "\n",
        "-   Note: [read about them](https://www.nature.com/articles/nrg1123) and\n",
        "    checkout\n",
        "    [haploview](https://www.broadinstitute.org/haploview/haploview) if\n",
        "    you don’t know what LD blocks are\n",
        "-   To make a block, we just replace the causal SNP with several SNPs in\n",
        "    varying LD\n",
        "-   Simplest way to do this is to draw from the uniform distribution for\n",
        "    r2\n",
        "-   The more complex is to use something like a half-normal distibution\n",
        "    that decreases as you move away from the causal SNP\n",
        "-   You’d think that it makes sense to slowly decrease LD as you move\n",
        "    physically away from the causal SNP\n",
        "-   In practice, this isn’t really the case, and drawing from a uniform\n",
        "    distribution does just fine"
      ],
      "id": "51924bf7-e8e5-43c4-94f0-a0636efc02f8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "add_simple_ld_block <- function(snp, block_size = 10) {\n",
        "  ld_values <- runif(block_size, 0.1, 0.9)\n",
        "  ld_block <- matrix(0, nrow = length(snp), ncol = block_size)\n",
        "\n",
        "  for (i in 1:block_size) {\n",
        "    ld_block[, i] <- add_simple_ld(snp, ld_values[i])\n",
        "  }\n",
        "\n",
        "  return(ld_block)\n",
        "}"
      ],
      "id": "2dca72f8-3efc-4acf-ae35-a95d47bfd343"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can test replacing a single causal SNP with a whole block of SNPs in\n",
        "LD and plot the correlation:"
      ],
      "id": "5b70bee2-7935-47d5-b10b-d65fe36d3c5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "ld_block <- add_simple_ld_block(g[, 1])\n",
        "ggcorrplot(cor(ld_block), type = \"lower\", colors = c(\"red\", \"white\", \"blue\"))"
      ],
      "id": "4c2f0266-9c8f-4c50-98c5-98ca51951718"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Success!\n",
        "\n",
        "We can then iterate through the causal SNPs in and replace each with a\n",
        "block. However, it’s going to up the dimensions substantially, so we’ll\n",
        "do two things:\n",
        "\n",
        "-   Re-simulate a smaller dataset with just 10 SNPs\n",
        "-   Randomly draw the block size from a distribution so we have\n",
        "    different block sizes"
      ],
      "id": "bc9b2c40-f831-452e-bd6e-84025dc7d5cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g <- add_noise(p = 10, 1000) # create initial dataset of 10 SNPs\n",
        "block_sizes <- sample(3:10, 10, replace = TRUE) # sample 10 blocks, each of which is 3-10 SNPs\n",
        "\n",
        "for (i in 1:ncol(g)) {\n",
        "  block <- add_simple_ld_block(g[, i], block_size = block_sizes[i])\n",
        "\n",
        "  if (i == 1) {\n",
        "    ld_blocks <- block\n",
        "  } else {\n",
        "    ld_blocks <- cbind(ld_blocks, block)\n",
        "  }\n",
        "}"
      ],
      "id": "ac6d81af-1e2c-4e36-ac04-e2941ac2c7e4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "ggcorrplot(cor(ld_blocks), type = \"lower\", colors = c(\"red\", \"white\", \"blue\"))"
      ],
      "id": "60fddf19-aff8-49e2-808d-d343b46cbcd2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now see some of the classic LD block structure from real\n",
        "populations! It’s worth noting a few points here though:\n",
        "\n",
        "-   The situation where you have two variants that are in high LD, but\n",
        "    only one of them is in high LD with a third variant, or where a\n",
        "    variant is in higher LD with a variant further away than the variant\n",
        "    right next to it is very common.\n",
        "-   Actual block structure varies by population (e.g. Europeans tend to\n",
        "    have a clearer block structure because of population\n",
        "    bottlenecks/inbreeding etc.)\n",
        "\n",
        "## Simulating even more realistic LD blocks\n",
        "\n",
        "What if we want to get even closer to realistic LD?\n",
        "\n",
        "### Background\n",
        "\n",
        "-   So far we’ve been simulating LD with the causal SNP, but it’s too\n",
        "    complicated to try to manually set the pairwise LD to be the same as\n",
        "    a real dataset using the method we’ve implemented\n",
        "-   This would require us iteratively shuffle predictors and keep\n",
        "    rechecking pairwise correlations between all the SNPs in a block\n",
        "    (and would ignore the fact that blocks aren’t that clean - there can\n",
        "    be LD with SNPs further away too!)\n",
        "-   More complicated pure-simulation methods approach the problem by\n",
        "    taking a population and a given evolutionary model and following\n",
        "    them forwards or backwards in time\n",
        "-   For example, they may start with a population with very little LD\n",
        "    (most variants are independent) and force it through population\n",
        "    bottlenecks to create LD blocks\n",
        "-   Alternatively, they may start with very strong LD blocks (most\n",
        "    variants dependent) and making the population undergo\n",
        "    mating/recombination to break up the LD blocks till LD structure\n",
        "    reaches the desired level\n",
        "-   These methods are typically more complicated to implement than what\n",
        "    we’ve done here - they may represent both chromosomes to allow for\n",
        "    crossing-over during meiosis, for example\n",
        "-   A classic example is\n",
        "    [GenomeSIMLA](https://link.springer.com/chapter/10.1007/978-3-540-78757-0_3),\n",
        "    but there are [a huge number of options these\n",
        "    day](https://onlinelibrary.wiley.com/doi/full/10.1002/gepi.21876)\n",
        "-   Realistically though, these methods can all be computationally\n",
        "    intensive, and aren’t used that often\n",
        "-   Usually, when we’re not too bothered about closely mimicking a real\n",
        "    population, it’s easier to simulate yourself, as we do here, so you\n",
        "    can precisely control the aspects you want that may not be available\n",
        "    in a given package\n",
        "-   When we want more realistic LD structure, it’s far easier (and\n",
        "    probably more realistic) to just take it from real data\n",
        "\n",
        "### Working with 1000 genomes (1kg) data\n",
        "\n",
        "We do this by sampling from real data (1kg), so LD structures are\n",
        "realistic and then we just need to create a phenotype.\n",
        "\n",
        "We can extract specific regions from 1kg remotely using a system call to\n",
        "bcftools, as below. If bcftools is a struggle to install, and you have\n",
        "the space, you can download biallelic SNV 1kg data from\n",
        "[here](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20181203_biallelic_SNV/)\n",
        "instead. We’ll be using a small subset of [the full 1kg\n",
        "data](https://www.internationalgenome.org/data/) - the APOE region from\n",
        "chromosome 19 only. Note that the genome build we’ll use (now and\n",
        "always) will be GRCh38. If you use the bcftools option, make sure you\n",
        "follow all the instructions on the download page and add the\n",
        "installation directory to your path. Using the method below will only\n",
        "download a filtered version of the APOE region locally as a vcf, which\n",
        "should be around 60Mb on disk, so not too big."
      ],
      "id": "50edecad-2d5c-4714-9c5b-31e3d7267172"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning in dir.create(here::here(\"data\")):\n",
            "'/Users/seafloor/genetic_simulations/data' already exists"
          ]
        }
      ],
      "source": [
        "# assuming you're working in the genetic_simulations directory\n",
        "# please username/directory structure and change slashes in paths if working in Windows\n",
        "\n",
        "dir.create(here::here(\"data\"))"
      ],
      "id": "58c87d29-1a2b-4b63-8576-b2e82936c042"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rows: 5840 Columns: 2557\n",
            "── Column specification ────────────────────────────────────────────────────────\n",
            "Delimiter: \"\\t\"\n",
            "chr (2556): #CHROM, ID, REF, ALT, QUAL, FILTER, INFO, FORMAT, HG00096, HG000...\n",
            "dbl    (1): POS\n",
            "\n",
            "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
            "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n",
            "`.name_repair` is omitted as of tibble 2.0.0.\n",
            "ℹ Using compatibility `.name_repair`."
          ]
        }
      ],
      "source": [
        "# read our new vcf file, which is variants * individuals\n",
        "vcf <- read_tsv(here::here(\"data\", \"apoe_region_1kg_grch38.vcf\"), comment = \"##\") %>%\n",
        "  mutate(ID = str_c(`#CHROM`, \"_\", POS, \"_\", REF, \"_\", ALT)) %>%\n",
        "  select(-`#CHROM`, -POS, -REF, -ALT, -QUAL, -FILTER, -INFO, -FORMAT)"
      ],
      "id": "c9ec052a-f9e3-4984-81f8-2607fe312969"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have a chunk of 1kg in the right format (counts of the ALT\n",
        "allele). We can run a quick check for MAF to make sure everything has\n",
        "gone well with reading, transforming and counting."
      ],
      "id": "c3461b68-fb8d-4dc5-9109-0836e7526c3c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Minimum MAF:  0\""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"rs7412 MAF (0.078 in TOPMED):  0.08\""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"rs429358 MAF (0.155 in TOPMED):  0.05\""
          ]
        }
      ],
      "source": [
        "get_maf <- function(v) {\n",
        "  afreq <- sum(v) / (2 * nrow(df))\n",
        "\n",
        "  if (afreq > 0.5) {\n",
        "    return(1 - afreq)\n",
        "  } else {\n",
        "    return(afreq)\n",
        "  }\n",
        "}\n",
        "\n",
        "apoe_maf <- sapply(df, get_maf)\n",
        "\n",
        "# check minimum\n",
        "print(paste(\"Minimum MAF: \", min(apoe_maf)))"
      ],
      "id": "b96d81ca-514f-49ce-991e-3166a6db7c18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "hist(apoe_maf, breaks = 20)"
      ],
      "id": "ae3b291c-8886-45f2-b138-4e19c9b7d0db"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a really big sample of variants now (too big to really show LD\n",
        "for all easily, so we can plot a subset to check the local LD). We had\n",
        "roughly 100 SNPs in our simulations of our own LD, but that was for\n",
        "seemingly small blocks of LD. We’ll just take the first 200 from the\n",
        "APOE region here."
      ],
      "id": "24f3517b-6a79-428b-9d0c-558f980a12fd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "ggcorrplot(cor(setNames(df[, 1:200], c()),\n",
        "               use = \"pairwise.complete.obs\",\n",
        "               method = \"pearson\"),\n",
        "           type = \"lower\",\n",
        "           colors = c(\"red\", \"white\", \"blue\"))"
      ],
      "id": "35351dc9-bcb3-4b1a-9dd0-d22f933a918e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, there aare definitely blocks, but the correlation\n",
        "structure is significantly more complicated!\n",
        "\n",
        "### Simulations on 1kg data\n",
        "\n",
        "Now we have our data and have checked the LD, we can create a phenotype.\n",
        "For now we’ll use an additive model for simplicity, but in future we can\n",
        "use our other methods to create interactions.\n",
        "\n",
        "For this we’ll run simulations under an additive liability-threshold\n",
        "model, fixing heritability and prevalence beforehand, as before. Unlike\n",
        "last time, we’ll set the noise in the model based on the simulated\n",
        "heritability we set. We’ll also randomly select 1% as causal variants\n",
        "from the dataset, so only a subset affect disease status, but there are\n",
        "many variants in LD with them."
      ],
      "id": "a1063e8a-600b-4688-a9e7-7079ac59af4e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simulate_y <- function(X, p_causal = 0.01, h2_l = 0.2, k = 0.2) {\n",
        "  m <- as.integer(ncol(X) * p_causal) # number of causal variants\n",
        "  betas_causal <- rnorm(m) # get betas for causal variants\n",
        "  betas_null <- rep(0, ncol(X) - m) # null effects for remaining variants\n",
        "  betas <- sample(c(betas_causal, betas_null), ncol(X), replace = FALSE) # shuffle the betas so causal SNPs are randomly distributed\n",
        "\n",
        "  # derive the genotypic risk from the causal SNPs\n",
        "  causal_bool <- !dplyr::near(betas, 0) # get index of causal SNPs\n",
        "  g <- as.matrix(X[, causal_bool]) %*% betas[causal_bool] # genotypic value\n",
        "  var_g <- var(g)\n",
        "\n",
        "  # derive the noise (environmental risk, e) to be big enough that we achieve\n",
        "  # the desired deritability in the final phenotype\n",
        "  var_e <- (var_g / h2_l) - var_g # variance in e (noise) based on variance in g (genotypic risk) and heritability on the liability scale\n",
        "  e <- rnorm(nrow(X), 0, sqrt(var_e)) # simulate error for phenotype for all individuals\n",
        "\n",
        "  # combine genotypic risk and noise as we have previously\n",
        "  l <- g + e\n",
        "  l_z <- (l - mean(l)) / sd(l)\n",
        "  threshold <- qnorm(1 - k)\n",
        "  p <- as.integer(l_z > threshold)\n",
        "\n",
        "  # we're going to cheat and return two things as a list, but generally\n",
        "  # a function should return one value\n",
        "  return(list(p, causal_bool))\n",
        "}"
      ],
      "id": "f9717298-a5e4-401c-9c31-2a186f5021de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We should now have a simulated phenotype for our dataset! They quick way\n",
        "to check is to fit a model.\n",
        "\n",
        "### Fitting sparse models on our simulations\n",
        "\n",
        "We’ll make use of our list of causal SNPs to check if they roughly match\n",
        "up with the most important SNPs in the model."
      ],
      "id": "01e8411f-4d2d-4833-98ae-ecd39c4ddbb2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get train-test split\n",
        "train_idx <- sample(1:nrow(df), as.integer(nrow(df) * 0.8))\n",
        "test_idx <- setdiff(1:nrow(df), train_idx)\n",
        "\n",
        "# setup simulation and store index for causal SNPs\n",
        "results <- simulate_y(df)\n",
        "X <- as.matrix(df)\n",
        "y <- results[[1]]\n",
        "causal_snps <- results[[2]]\n",
        "\n",
        "# fit a lasso model\n",
        "grid_params <- 10^seq(10, -2, length = 100)\n",
        "X_train <- X[train_idx, ]\n",
        "y_train <- y[train_idx]\n",
        "mod_lasso <- glmnet(X_train, y_train,\n",
        "                    family = \"binomial\", alpha = 1,\n",
        "                    lambda = grid_params)"
      ],
      "id": "b1a62ead-698f-4fff-9469-9cc15d191a5e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see the plot below which shows us the coefficients for the\n",
        "different L1 norms, and that some of the coefficients do indeed drop to\n",
        "zero, so we can fit a nice sparse model to the data."
      ],
      "id": "9a982d73-20e2-4852-979e-fede908360f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "plot(mod_lasso)"
      ],
      "id": "77641419-d3e1-4045-a9e0-3868df8eca11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But first we need to find the best lambda value to use for our model. We\n",
        "can do this using cross-validation. In reality, for such a small\n",
        "dataset, we would probably want to run nested cross-validation, but for\n",
        "now we’ll just use the whole dataset and run a train-test split as\n",
        "above. The practical consequence of this is that any estimate we have of\n",
        "predictive performance will be slightly off (likely overly-optimistic)."
      ],
      "id": "c7fccdc1-fff4-45f2-bd9d-8e70e4754281"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use cross-validation to find the best lambda\n",
        "cv_out <- cv.glmnet(X_train, y_train, alpha = 1,\n",
        "                    parallel = TRUE, nfolds = 5,\n",
        "                    family = \"binomial\")\n",
        "\n",
        "best_lambda <- cv_out$lambda.min\n",
        "\n",
        "# predict from our fit model with the best lambda\n",
        "X_test <- X[test_idx, ]\n",
        "y_test <- y[test_idx]\n",
        "test_lasso <- predict(mod_lasso, s = best_lambda,\n",
        "                      type=\"response\",\n",
        "                      newx = X_test)"
      ],
      "id": "bb1b1a53-57b9-4609-a65b-99e9db89f35c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"N top predictors: 68, N expected: 58\""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Overlap: 7 out of 58\""
          ]
        }
      ],
      "source": [
        "lasso_coef <- predict(mod_lasso, type = \"coefficients\",\n",
        "                      s = best_lambda)\n",
        "\n",
        "# get the top predictors\n",
        "top_predictors <- lasso_coef@Dimnames[[1]][lasso_coef@i]\n",
        "\n",
        "# get the list of simulated causal SNPs\n",
        "top_predictors_expected <- colnames(df)[causal_snps]\n",
        "\n",
        "# check the counts of top predictors\n",
        "print(paste(\"N top predictors: \", length(top_predictors),\n",
        "            \", N expected: \", length(top_predictors_expected),\n",
        "            sep = \"\"))"
      ],
      "id": "6eabde05-b2f9-4fa3-8521-0830d3d61126"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, we didn’t do very well in find the most important SNPs in\n",
        "this dataset. LD raises a substantial problem for machine learning\n",
        "models. Well, any model, including GWAS, but multivariable models have\n",
        "been relatively neglected compared to univariable approaches in\n",
        "association studies, so we have fewer tools for figuring out the causal\n",
        "variants. We also chose a region with really strong LD structure and\n",
        "didn’t perform any pruning or clumping (filtering the variants by LD),\n",
        "so it’s somewhat unsurprising that we didn’t do well.\n",
        "\n",
        "Finally we can check our AUC quickly to see how well we’ve done in terms\n",
        "of prediction."
      ],
      "id": "9d454065-2491-405a-9f7f-d79bf365fa62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Call:\n",
            "roc.default(response = y_test, predictor = as.vector(test_lasso))\n",
            "\n",
            "Data: as.vector(test_lasso) in 406 controls (y_test 0) < 104 cases (y_test 1).\n",
            "Area under the curve: 0.6723"
          ]
        }
      ],
      "source": [
        "pROC::roc(y_test, as.vector(test_lasso))"
      ],
      "id": "22ad0491-7a6b-4b55-9736-5a2439aac6f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Despite not finding the causal SNPs, we’ve done quite well in terms of\n",
        "predictions. Again, not a suprising result. There’s strong LD structure,\n",
        "so we can predict well from variants in LD with the causal SNPs. This\n",
        "seems okay on the surface, but as the LD between the causal SNP and the\n",
        "SNP in our model vary across global populations, we would likely see a\n",
        "substantial drop in predictive performance in a different population.\n",
        "This is a fundamental problem with genetic prediction models.\n",
        "\n",
        "## Summary\n",
        "\n",
        "We’ve covered a lot of ground here, but the main points are: - We can\n",
        "simulate very simple LD and even block structures a few lines of code -\n",
        "We can use real data and simulate phenotypes from it - We can then fit\n",
        "ML models to this data, but - LD presents a significant issue for\n",
        "identifying the causal variant in a dataset\n",
        "\n",
        "There are a few logical follow-on questions. First is whether different\n",
        "ML models can perform better in such a scenario. Second is, given PRS\n",
        "often out-perform ML methods on complex trait prediction, is this\n",
        "largely due to how they handle LD?"
      ],
      "id": "a88f9559-9359-421a-8c9b-d4f14d7b5acc"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}